{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SzymonNowakowski/Machine-Learning-2025/blob/master/Lab15-EDM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 15 - EDM, Working in ICM\n",
        "\n",
        "### Author: Szymon Nowakowski"
      ],
      "metadata": {
        "id": "xl_-W_aXqjJ2"
      },
      "id": "xl_-W_aXqjJ2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Today I will talk about executing code from the following paper:\n",
        "\n",
        "Karras, T., Aittala, M., Aila, T., & Laine, S. (2022). Elucidating the Design Space of Diffusion-Based Generative Models. In Proceedings of the Neural Information Processing Systems (NeurIPS) Conference.\n",
        "https://arxiv.org/abs/2206.00364\n",
        "\n",
        "It has 2.7k citations as of October 2025.\n",
        "\n",
        "### EDM Codebase\n",
        "\n",
        "The code itself can be found here:\n",
        "\n",
        "https://github.com/NVlabs/edm\n",
        "\n",
        "Although 3 years old, the codebase and the precomputed networks it provides are still considered the state-of-the-art for generating images."
      ],
      "metadata": {
        "id": "q90PnkdyThfK"
      },
      "id": "q90PnkdyThfK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Presentation Plan\n",
        "\n",
        "## Cluster Computation (ICM Example)\n",
        "\n",
        "## Working with Containers\n",
        "\n",
        "## Best (Not Only Programming) Practices\n",
        "\n",
        "## EDM\n",
        "\n",
        "### Pretrained Networks\n",
        "\n",
        "### Generating Images\n",
        "\n",
        "### Calculating FID"
      ],
      "metadata": {
        "id": "Ceu8ykzgYSu4"
      },
      "id": "Ceu8ykzgYSu4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cluster Computation (ICM Example)"
      ],
      "metadata": {
        "id": "UZa709sKjXCr"
      },
      "id": "UZa709sKjXCr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## SLURM\n",
        "\n",
        "In ICM they use ***SLURM***. SLURM stands for **Simple Linux Utility for Resource Management** — it's a workload manager widely used on **HPC (High-Performance Computing)** clusters to schedule and manage jobs.\n",
        "\n",
        "It handles job submission, queuing, scheduling, monitoring, and resource allocation.\n",
        "\n",
        "In SLURM each command starts with a letter `s`:\n",
        "\n",
        "    {bash}\n",
        "    # Submit a job\n",
        "    sbatch myjob.sh\n",
        "\n",
        "    # Check job queue\n",
        "    squeue -u $USER\n",
        "\n",
        "    # Check all jobs\n",
        "    squeue\n",
        "\n",
        "    # Cancel a job\n",
        "    scancel 12345\n",
        "\n",
        "\n",
        "I will show you an example of a SLURM batch job file  (`myjob.sh` in examples above) in a moment.\n",
        "\n",
        "Dicussing SLURM in detail exceeds the scope of this class:\n",
        "\n",
        "- The great resource I recommend (and use often myself) is the [ICM help page on SLURM](https://kdm.icm.edu.pl/Tutorials/HPC-intro/slurm_intro/).\n",
        "\n",
        "- The available resouces in ICM i.e. server configurations, installed GPUs, available memory can be checked at the [ICM help page on computing resources](https://kdm.icm.edu.pl/Zasoby/komputery_w_icm.pl/).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UWP80vZyaNMa"
      },
      "id": "UWP80vZyaNMa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSF\n",
        "\n",
        "\n",
        "There exist other computing HPC environments, the most popular alternative to SLURM being ***LSF*** (**Load Sharing Facility**), developed originally by Platform Computing (currently delivered by IBM).\n",
        "\n",
        "In LSF each command starts with a letter `b`:\n",
        "\n",
        "    {bash}\n",
        "    # Submit a job\n",
        "    bsub < myjob.lsf\n",
        "\n",
        "    # Check your jobs\n",
        "    bjobs\n",
        "\n",
        "    # Check all jobs\n",
        "    bjobs -u all\n",
        "\n",
        "    # Show information about available queues\n",
        "    bqueues\n",
        "\n",
        "    # Kill a job\n",
        "    bkill 12345\n"
      ],
      "metadata": {
        "id": "UXTc8oHrjT6n"
      },
      "id": "UXTc8oHrjT6n"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with Containers\n",
        "\n",
        "In many HPC environments (including ICM), it is hard to install your own dependencies. While it is possible to install additional Python packages, it is sometimes hard or impossible to install additionall binaries.\n",
        "\n",
        "In another cluster I work with in WIM, there is no Internet availability at all, so one cannot even install additional packages.\n",
        "\n",
        "**The way out is using the containers.** But there is another obstacle: you cannot run Docker directly for security reasons in a cluster.  \n",
        "\n",
        "Instead, you typically **build docker containers locally** (on your workstation or laptop) and then **transfer them** to the cluster, where they can be rebuilt to *Apptainer* (formerly Singularity) format.\n",
        "\n",
        "Below are the steps to create, save, and rebuild a Docker container image."
      ],
      "metadata": {
        "id": "VbufEDhgozJY"
      },
      "id": "VbufEDhgozJY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## `Dockerfile`\n",
        "\n",
        "The `Dockerfile` describes your environment — e.g., which base image to use, which Python packages to install, etc.\n",
        "\n",
        "    {bash}\n",
        "    # Example Dockerfile\n",
        "    FROM pytorch/pytorch:2.3.0-cuda12.1-cudnn8-runtime\n",
        "\n",
        "    # Set working directory\n",
        "    WORKDIR /workspace\n",
        "\n",
        "    # Copy your project\n",
        "    COPY . /workspace\n",
        "\n",
        "    # Install extra dependencies\n",
        "    RUN pip install -r requirements.txt\n",
        "\n",
        "    # Default command\n",
        "    CMD [\"python\", \"train.py\"]\n",
        "\n"
      ],
      "metadata": {
        "id": "RIDQyXrFo1ZR"
      },
      "id": "RIDQyXrFo1ZR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Docker Image\n",
        "\n",
        "    {bash}\n",
        "    docker build -t myproject:latest .\n",
        "\n",
        "This creates a local image named `myproject:latest`. Note the `.` (dot) indicating the `Dockerfile` file in the current directory.\n",
        "\n",
        "Now, when you run\n",
        "\n",
        "    {bash}\n",
        "    docker run myproject\n",
        "what happens is that the `python train.py` gets executed within the container.\n",
        "\n",
        "There can only be one `CMD` instruction in a `Dockerfile` (if there are multiple, only the last one is used).\n",
        "\n",
        "Often, `CMD` is used to launch the main script or application of the container. This command can be overwritten and you can run some other code (you can even pass arguments) by executing\n",
        "\n",
        "    {bash}\n",
        "    docker run myproject python some_other_code.py --epochs 10 --lr 1e-3"
      ],
      "metadata": {
        "id": "NTvBbZrlo5pg"
      },
      "id": "NTvBbZrlo5pg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the Image to a `.tar` File\n",
        "\n",
        "    {bash}\n",
        "    docker save myproject:latest -o myproject_latest.tar\n",
        "\n",
        "\n",
        "What happens is that the file `myproject_latest.tar` gets written to the current directory."
      ],
      "metadata": {
        "id": "IorczX41pPdi"
      },
      "id": "IorczX41pPdi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Copying the container image to ICM\n",
        "\n",
        "Use `scp` (secure copy) to transfer the image file to your home directory at ICM.  \n",
        "Replace `username` with your ICM login name.\n",
        "\n",
        "    {bash}\n",
        "    scp myproject_latest.tar username@hpc.icm.edu.pl:/lu/tetyda/home/username/\n",
        "\n",
        "`/lu/tetyda/home/username/` is a global path of your rysy home directory visible from `hpc.icm.edu.pl` server.\n"
      ],
      "metadata": {
        "id": "caBfxZDVljIm"
      },
      "id": "caBfxZDVljIm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting the Image to Apptainer Format on ICM\n",
        "\n",
        "Here is an example of the SLURM configuration file that will rebuild the docker container `myproject_latest.tar` into apptainer's `myproject_latest.sif`:\n",
        "\n",
        "    {bash}\n",
        "    # after `ssh`-ing to RYSY\n",
        "    more rebuild_container.slurm\n",
        "\n",
        "    #!/bin/bash\n",
        "    #SBATCH --job-name=docker2apptainer\n",
        "    #SBATCH --nodes=1\n",
        "    #SBATCH --ntasks=1\n",
        "    #SBATCH --gres=gpu:1\n",
        "    #SBATCH --time=48:00:00\n",
        "    #SBATCH --account=g99-4302\n",
        "    #SBATCH --output=slurm-%j.out\n",
        "\n",
        "    export APPTAINER_TMPDIR=/home/$USER/tmp\n",
        "\n",
        "    apptainer build ./myproject_latest.sif docker-archive:///home/$USER/myproject_latest.tar\n",
        "\n",
        "Now we shall go through it line by line.\n",
        "\n",
        "- `export APPTAINER_TMPDIR=/home/$USER/tmp` - this lines ensures that there is enough space on the output for temporary fies. Obviously, you need to create the `~/tmp` directory first.\n",
        "- Note the triple `///` - it costed me a few days to figure it out. These days, ChatGPT can supply such hints instantly.\n",
        "\n",
        "Now one needs to execute on RYSY\n",
        "\n",
        "    {bash}\n",
        "    sbatch rebuild_container.slurm\n",
        "    squeue\n",
        "\n",
        "to see something like\n",
        "\n",
        "    {bash}\n",
        "    JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
        "    127248       gpu setup_en   ljanis PD       0:00      1 (AssocGrpCPUMinutesLimit)\n",
        "    134837       gpu      OLA tomaszsu  R 12-08:46:28     1 rysy-n6\n",
        "    134838       gpu docker2a   szymon  R       0:18      1 rysy-n1\n",
        "    135515        ve     bash   herman  R    8:13:47      1 pbaran\n",
        "\n",
        "And after a few minutes or hours, depending on the docker file size, one gets `myproject_latest.sif` file written to the current directory. **This is the apptainer container file**.\n",
        "\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "mxvrzmorqU1L"
      },
      "id": "mxvrzmorqU1L"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the EDM Dockerfile\n",
        "\n",
        "The original EDM `Dockerfile` from `https://github.com/NVlabs/edm/blob/main/Dockerfile` is the following\n",
        "\n",
        "    {bash}\n",
        "    # Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
        "    #\n",
        "    # This work is licensed under a Creative Commons\n",
        "    # Attribution-NonCommercial-ShareAlike 4.0 International License.\n",
        "    # You should have received a copy of the license along with this\n",
        "    # work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n",
        "\n",
        "    FROM nvcr.io/nvidia/pytorch:22.10-py3\n",
        "\n",
        "    ENV PYTHONDONTWRITEBYTECODE 1\n",
        "    ENV PYTHONUNBUFFERED 1\n",
        "\n",
        "    RUN pip install imageio imageio-ffmpeg==0.4.4 pyspng==0.1.0\n",
        "\n",
        "    WORKDIR /workspace\n",
        "\n",
        "    RUN (printf '#!/bin/bash\\nexec \\\"$@\\\"\\n' >> /entry.sh) && chmod a+x /entry.sh\n",
        "    ENTRYPOINT [\"/entry.sh\"]\n",
        "\n",
        "The original container file gave me errors (I was not able to execute the code from within it,  no doubt due to broken dependencies along the way - the code is 3 years old), and I had no access to the original NVidia image which would have obviously worked fine. I needed to update `pillow` package in my fork **`https://github.com/SzymonNowakowski/edm/blob/main/Dockerfile`:**\n",
        "\n",
        "    {bash}\n",
        "    # Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
        "    #\n",
        "    # This work is licensed under a Creative Commons\n",
        "    # Attribution-NonCommercial-ShareAlike 4.0 International License.\n",
        "    # You should have received a copy of the license along with this\n",
        "    # work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n",
        "\n",
        "    FROM nvcr.io/nvidia/pytorch:22.10-py3\n",
        "\n",
        "    ENV PYTHONDONTWRITEBYTECODE 1\n",
        "    ENV PYTHONUNBUFFERED 1\n",
        "\n",
        "    RUN pip install imageio imageio-ffmpeg==0.4.4 pyspng==0.1.0\n",
        "    RUN pip install --upgrade pillow\n",
        "\n",
        "    WORKDIR /workspace\n",
        "\n",
        "    RUN (printf '#!/bin/bash\\nexec \\\"$@\\\"\\n' >> /entry.sh) && chmod a+x /entry.sh\n",
        "    ENTRYPOINT [\"/entry.sh\"]\n",
        "\n",
        "From there the standard sequence would let me build the docker container and ship it to ICM:\n",
        "\n",
        "    {bash}\n",
        "    docker build -t edm:latest .\n",
        "    docker save edm:latest -o edm_latest.tar\n",
        "    scp edm_latest.tar username@hpc.icm.edu.pl:/lu/tetyda/home/username/\n",
        "\n",
        "Now we need to rewrite the SLURM script to include different filenames (it assumes the `~/tmp` directory is available - if not - create it!) so it looks like this:\n",
        "\n",
        "    {bash}\n",
        "    # while on your terminal\n",
        "    ssh username@hpc.icm.edu.pl\n",
        "\n",
        "    # while on HPC computer\n",
        "    ssh rysy\n",
        "\n",
        "    # while on RYSY computer\n",
        "    more rebuild_container.slurm\n",
        "    #!/bin/bash\n",
        "    #SBATCH --job-name=docker2apptainer\n",
        "    #SBATCH --nodes=1\n",
        "    #SBATCH --ntasks=1\n",
        "    #SBATCH --gres=gpu:1\n",
        "    #SBATCH --time=48:00:00\n",
        "    #SBATCH --account=g99-4302\n",
        "    #SBATCH --output=slurm-%j.out\n",
        "\n",
        "    export APPTAINER_TMPDIR=/home/$USER/tmp\n",
        "\n",
        "    apptainer build ./edm_latest.sif docker-archive:///home/szymon/edm_latest.tar\n",
        "\n",
        "Now one needs to execute\n",
        "\n",
        "    {bash}\n",
        "    sbatch rebuild_container.slurm\n",
        "    squeue\n",
        "\n",
        "to see something like\n",
        "\n",
        "    {bash}\n",
        "    JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
        "    127248       gpu setup_en   ljanis PD       0:00      1 (AssocGrpCPUMinutesLimit)\n",
        "    134837       gpu      OLA tomaszsu  R 12-08:46:28     1 rysy-n6\n",
        "    134839       gpu docker2a   szymon  R       0:18      1 rysy-n1\n",
        "    135515        ve     bash   herman  R    8:13:47      1 pbaran\n",
        "\n",
        "And after a few hours the file `edm_latest.sif` gets written to the current directory.\n",
        "\n",
        "**This is the apptainer container file we shall need**."
      ],
      "metadata": {
        "id": "zUs4zB_uMSGi"
      },
      "id": "zUs4zB_uMSGi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best (Not Only Programming) Practices"
      ],
      "metadata": {
        "id": "D8Z0owLY-2o7"
      },
      "id": "D8Z0owLY-2o7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Configure Your `ssh` and `scp` Connection\n",
        "\n",
        "As you need a stable `ssh` and `scp` connection with the cluster, you will need to configure it with a file `~/.ssh/config`. An example of such a config file I use:\n",
        "\n",
        "    {bash}\n",
        "    cat ~/.ssh/config\n",
        "\n",
        "    Host icm\n",
        "        ForwardX11 yes\n",
        "        ForwardAgent yes\n",
        "        UserKnownHostsFile ~/.ssh/known_hosts\n",
        "        Hostname hpc.icm.edu.pl\n",
        "        LocalForward 8022 rysy:22\n",
        "        ServerAliveInterval 120\n",
        "        ServerAliveCountMax 2\n",
        "        User szymon\n",
        "\n",
        "    Host fizyk1.fuw.edu.pl\n",
        "        Hostname fizyk1.fuw.edu.pl\n",
        "        ServerAliveInterval 120\n",
        "        ServerAliveCountMax 2\n",
        "        User snowakowski\n",
        "\n",
        "    Host dg1\n",
        "        ForwardX11 yes\n",
        "        ForwardAgent yes\n",
        "        UserKnownHostsFile ~/.ssh/known_hosts\n",
        "        Hostname 10.21.2.118\n",
        "        LocalForward 5900 localhost:5900\n",
        "        LocalForward 8000 localhost:8000\n",
        "        LocalForward 8001 localhost:8001\n",
        "        LocalForward 8008 localhost:8008\n",
        "        LocalForward 8888 localhost:8888\n",
        "        LocalForward 6006 localhost:6006\n",
        "        LocalForward 1433 localhost:1433\n",
        "        RequestTTY yes\n",
        "        ServerAliveInterval 120\n",
        "        ServerAliveCountMax 2\n",
        "        ProxyJump  fizyk1.fuw.edu.pl:22\n",
        "        User szym\n",
        "\n",
        "- `ServerAliveInterval 120` - instructs your SSH client to send a small, encrypted keep-alive message to the server every 120 seconds (2 minutes).\n",
        "So every 2 minutes, your client pings the server silently to say “I'm still here.”\n",
        "\n",
        "- `ServerAliveCountMax 2` - defines how many unanswered keep-alive messages the client will tolerate before disconnecting.\n",
        "With `ServerAliveCountMax 2`, if the server fails to respond to two consecutive keep-alives, the client assumes the connection is dead and terminates it.\n",
        "\n",
        "The `dg1` via `fizyk1` connection is to show you how to configure the proxy jump.\n",
        "\n",
        "Now, instead of\n",
        "\n",
        "    {bash}\n",
        "    # while on your terminal\n",
        "    ssh -l snowakowski fizyk1.fuw.edu.pl\n",
        "\n",
        "    # while on fizyk1\n",
        "    ssh -l szym 10.21.2.118\n",
        "\n",
        "You can invoke simply one-step connection and with a server name (instead of the IP) and without a username!\n",
        "\n",
        "    {bash}\n",
        "    ssh dg1\n",
        "\n",
        "The same applies to `scp`.\n",
        "\n",
        "**Next I will show you how to enable the tab-completion of remote paths in `scp`**.\n"
      ],
      "metadata": {
        "id": "8l-DphqarxQp"
      },
      "id": "8l-DphqarxQp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Configure Your `ssh` and `scp` Certificate\n",
        "\n",
        "As you will frequently need to `ssh` and `scp` to and from the cluster, the second thing to consider is to set up a password-less connection. You not only skip typing your password every time, but you also unlock extra conveniences such as **tab-completion of remote paths** in `scp`.\n",
        "\n",
        "To generate a public/private key pair execute:\n",
        "\n",
        "    {bash}\n",
        "    ssh-keygen\n",
        "\n",
        "It generates:\n",
        "- your private key: `~/.ssh/id_rsa`\n",
        "- your public key: `~/.ssh/id_rsa.pub`\n",
        "\n",
        "To copy the public key to the server (for instance to ICM) do it with\n",
        "\n",
        "    {bash}\n",
        "    ssh-copy-id icm\n",
        "\n",
        "(assuming you have your `~/.ssh/config` file already created).\n",
        "\n",
        "After that\n",
        "\n",
        "    {bash}\n",
        "    ssh icm\n",
        "\n",
        "gets you to the `hpc.icm.edu.pl` server with only the OTP (one time password). That you cannot avoid, or at least I don't know how. In `hpc`, you can set up a direct certificate-based (pasword-less) connection to `rysy`, to be able to type only (without the password).\n",
        "\n",
        "    {bash}\n",
        "    ssh rysy"
      ],
      "metadata": {
        "id": "GvwDdgXq-5sm"
      },
      "id": "GvwDdgXq-5sm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Smart Use of GitHub\n",
        "\n",
        "Using GitHub is always a good idea — it provides independent backup, transparent version control, and collaboration features with very little extra effort.  \n",
        "\n",
        "While demonstrating how to use `git` in detail is beyond the scope of this class, there are a few important practices worth mentioning.\n",
        "\n",
        "- Since we want to extend the EDM repository from `https://github.com/NVlabs/edm`, it is best to **create your own fork** (as I did with `https://github.com/SzymonNowakowski/edm`).  \n",
        "  This allows you to modify the code freely - for instance, by adding a new `Dockerfile` instruction - and push those changes safely to your own repository.\n",
        "\n",
        "- With `git`, every time you run your code you are working on a **specific, uniquely identified version** that can always be restored.  \n",
        "  The *commit hash* serves as that unique version identifier, and using just the first 6–10 characters is typically sufficient.\n",
        "\n",
        "- You can automatically retrieve the current version hash using a small Python helper:\n",
        "\n",
        "  ```{Python}\n",
        "  def get_git_revision_short_hash() -> str:\n",
        "      return (\n",
        "          subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD'])\n",
        "          .decode('ascii')\n",
        "          .strip()\n",
        "      )\n",
        "\n",
        "  ```\n",
        "  This function reads the current Git commit hash, converts it from bytes to a string, and trims the trailing newline character.\n",
        "\n",
        "- You can prefix this short hash to all outputs — file names, directories, or logs. This guarantees that results can always be reproduced: simply retrieve the same code version and rerun it on the same data.\n",
        "\n",
        "- Sometimes I even use two version identifiers: one for the code that trained the neural network, and another for the code version used to test or evaluate it.\n",
        "\n",
        "- **The real issue is forgetting** — you must remember to commit your changes before executing the code.\n",
        "\n",
        "- **Working in a remote environment actually helps prevent that**. When developing locally (e.g., in PyCharm) and running jobs on ICM, the proper workflow is to synchronize code using `git push` and `git pull`, not `scp`. This way, **forgetting to commit becomes impossible** — if you don't push and pull your latest version, the remote host will simply run the old code!\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i6lPkN5rR98u"
      },
      "id": "i6lPkN5rR98u"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDM"
      ],
      "metadata": {
        "id": "SMpFhnM8SHSp"
      },
      "id": "SMpFhnM8SHSp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pretrained Networks"
      ],
      "metadata": {
        "id": "VzteE6wrcAGY"
      },
      "id": "VzteE6wrcAGY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parametrization used\n",
        "\n",
        "The usual way to train a neural network in the diffusion papers is by generating samples\n",
        "$$\n",
        "Y_t = \\alpha_t X + \\sigma_t \\epsilon,\n",
        "$$\n",
        "with (for all possible $t$ we consider):\n",
        "- $\\alpha_t$ the signal schedule,\n",
        "- $\\sigma_t$ the noise schedule,\n",
        "- $\\epsilon \\sim N(0,1)$.\n",
        "\n",
        "In EDM they use $\\alpha \\equiv 1$. They use the denoiser $D$ estimating the true image $X$ from the noised image $X+\\sigma \\epsilon$ with $\\epsilon \\sim N(0,1)$.\n",
        "\n",
        "$$\n",
        "\\hat X = D (X+\\sigma \\epsilon, \\sigma)\n",
        "$$\n",
        "\n",
        "However, $D$ is not a neural network. Rather, it is a function which uses a neural network $F$ and four fixed functions of $\\sigma$: $c_{skip}, c_{out}, c_{in}\\text{ and }c_{noise}$. With\n",
        "\n",
        "$$\n",
        "Y=X+\\sigma \\epsilon\n",
        "$$\n",
        "we have\n",
        "$$\n",
        "\\hat X = D (Y, \\sigma) := c_{skip}(\\sigma)Y + c_{out}(\\sigma) F \\left[ c_{in}(\\sigma) Y, c_{noise}(\\sigma) \\right]\n",
        "$$\n",
        "\n",
        "The four fixed functions of $\\sigma$: $c_{skip}, c_{out}, c_{in}\\text{ and }c_{noise}$ are chosen in such a way, that\n",
        " - the input of the $F$ network has unit variance: it fixes $c_{in}(\\sigma)$ which is applied before the input is passed to $F$:\n",
        "\n",
        "  $$\n",
        "  c_{in}(\\sigma) = \\frac{1}{\\sqrt{\\sigma^2 + \\sigma_{data}^2}}\n",
        "  $$\n",
        "- the output of the $F$ network has unit variance; thus, $c_{out}(\\sigma)$, applied after the network output, is expressed in terms of $c_{skip}(\\sigma)$:\n",
        "   $$\n",
        "    c_{out}(\\sigma)^2 = \\left(1- c_{skip}(\\sigma) \\right)^2 \\sigma_{data}^2 + c_{skip}(\\sigma)^2 \\sigma^2\n",
        "   $$\n",
        "\n",
        "- then the $c_{skip}$ function is selected so it minimizes $c_{out}(\\sigma)$ so the errors of $F$ get amplified as little as possible:\n",
        "\n",
        "  $$\n",
        "    c_{skip}(\\sigma) = \\frac{\\sigma_{data}^2}{\\sigma^2 + \\sigma_{data}^2},\n",
        "  $$\n",
        "  and\n",
        "  $$\n",
        "  c_{out}(\\sigma) = \\frac{\\sigma \\cdot \\sigma_{data}}{\\sqrt{\\sigma^2 + \\sigma_{data}^2}}\n",
        "  $$\n",
        "\n",
        "- $c_{noise}$ is selected empirically to be $c_{noise} = \\frac{1}{4}\\ln{\\sigma}$.\n",
        "\n",
        "See the Appendix B.6 of the Supplementary material to the paper for the derivations.\n",
        "\n",
        "When using the EDM code, you don't have direct access to $F$. You use the denoiser ($D$) directly.\n",
        "\n",
        "The denoiser ($D$) gets implemented in `EDMPrecond` class. You can see $F$ and the four functions we discussed applied to the input and output of $F$ [in the `forward` method of `EDMPrecond` class](https://github.com/NVlabs/edm/blob/main/training/networks.py#L654).\n",
        "\n",
        "From the code you'll also note, that the $c_{data}=0.5$. For most natural image datasets normalized to [-1, 1], you'll find the standard deviation to be close to 0.5, which is why it's used as the default. However, for other data types (audio, different normalization schemes, synthetic data), you might need to adjust this value based on your actual data statistics."
      ],
      "metadata": {
        "id": "qNZZHTX5_e0K"
      },
      "id": "qNZZHTX5_e0K"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Function\n",
        "\n",
        "\n",
        "Just for the completeness, EDM minimizes the loss computed in [the `EDMLoss` class](https://github.com/NVlabs/edm/blob/main/training/loss.py#L66) and expressed as\n",
        "\n",
        "$$\n",
        " \\mathbb{E}_{\\sigma, X\\sim data, \\epsilon \\sim N(0,1)} \\left[ \\lambda(\\sigma) \\cdot \\| D(X+\\sigma \\epsilon, \\sigma) - X\\| _2^2\\right],\n",
        "$$\n",
        "with the weight\n",
        "$$\n",
        "\\lambda(\\sigma) = \\frac{\\sigma ^ 2 + \\sigma_{data}^ 2}{\\sigma ^2 \\cdot \\sigma_{data}^2}\n",
        "$$\n",
        "\n",
        "You'll note that $\\sigma$ plays the role of time coordinate in their formulation (recall, that $\\alpha \\equiv 1$).\n",
        "\n",
        "The other parameter that must be chosen is the distribution of $\\sigma$. The EDM autors choose the lognormal distribution with\n",
        "\n",
        "$$\n",
        "  \\ln{\\sigma} \\sim N(\\mu = -1.2, \\sigma^2=1.2^2)\n",
        "$$\n",
        "\n",
        "With $\\ln(\\sigma) \\sim \\mathcal{N}(-1.2,\\, 1.2^2)$:\n",
        "\n",
        "**Median:**  \n",
        "$$\n",
        "\\mathrm{med}(\\sigma) = e^{-1.2} \\approx 0.301\n",
        "$$\n",
        "\n",
        "**Mean:**  \n",
        "$$\n",
        "\\mathbb{E}[\\sigma] = e^{-1.2 + 1.2^2 / 2} = e^{-0.48} \\approx 0.619\n",
        "$$\n",
        "\n",
        "The distribution covers roughly  \n",
        "$$\n",
        "\\sigma \\in [e^{-1.2 - 3(1.2)},\\, e^{-1.2 + 3(1.2)}] = [e^{-4.8},\\, e^{2.4}] \\approx [0.008,\\, 11.02]\n",
        "$$\n",
        "(i.e., within ±3 standard deviations in log-space).\n",
        "\n",
        "\n",
        "Also, recall that $c_{\\text{noise}} = \\ln\\sigma / 4$, so  \n",
        "$$\n",
        "c_{\\text{noise}} \\sim \\mathcal{N}(-0.3,\\, 0.3^2).\n",
        "$$"
      ],
      "metadata": {
        "id": "F8ixGziTcDN2"
      },
      "id": "F8ixGziTcDN2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pretrained Networks Provided\n",
        "\n",
        "The list of pretrained models can be accessed [here](https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/) and it shows:\n",
        "\n",
        "    {bash}\n",
        "    LICENSE.txt\t21359\t10/30/2022, 8:07:15 AM\n",
        "    _all_files.zip\t2859235624\t10/30/2022, 11:27:48 AM\n",
        "    edm-afhqv2-64x64-uncond-ve.pkl\t251340663\t10/30/2022, 11:14:22 AM\n",
        "    edm-afhqv2-64x64-uncond-vp.pkl\t247513130\t10/30/2022, 11:15:36 AM\n",
        "    edm-cifar10-32x32-cond-ve.pkl\t225848751\t10/30/2022, 11:15:51 AM\n",
        "    edm-cifar10-32x32-cond-vp.pkl\t223183453\t10/30/2022, 11:15:52 AM\n",
        "    edm-cifar10-32x32-uncond-ve.pkl\t225833012\t10/30/2022, 11:15:52 AM\n",
        "    edm-cifar10-32x32-uncond-vp.pkl\t223173327\t10/30/2022, 11:17:03 AM\n",
        "    edm-ffhq-64x64-uncond-ve.pkl\t251340661\t10/30/2022, 11:17:11 AM\n",
        "    edm-ffhq-64x64-uncond-vp.pkl\t247513128\t10/30/2022, 11:17:12 AM\n",
        "    edm-imagenet-64x64-cond-adm.pkl\t1183892888\t10/30/2022, 11:17:12 AM\n"
      ],
      "metadata": {
        "id": "nwhYoNhlcIsR"
      },
      "id": "nwhYoNhlcIsR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Images\n",
        "\n",
        "The images are generated in the function [`edm_smapler(...)` in `generate.py`](https://github.com/NVlabs/edm/blob/main/generate.py#L25).\n",
        "\n",
        "Below is the annotated version of the code, with comments explaining the purpose of each line to make it easier to follow.\n",
        "\n",
        "```{Python}\n",
        "def edm_sampler(\n",
        "    net, latents, class_labels=None, randn_like=torch.randn_like,\n",
        "    num_steps=18, sigma_min=0.002, sigma_max=80, rho=7,\n",
        "    S_churn=0, S_min=0, S_max=float('inf'), S_noise=1,\n",
        "):\n",
        "    # Adjust noise levels based on what's supported by the network.\n",
        "    # (Ensure sigmas stay within the range the network was trained for.)\n",
        "    sigma_min = max(sigma_min, net.sigma_min)\n",
        "    sigma_max = min(sigma_max, net.sigma_max)\n",
        "\n",
        "    # Time step discretization.\n",
        "    step_indices = torch.arange(num_steps, dtype=torch.float64, device=latents.device)\n",
        "    # Create indices [0, ..., num_steps-1] on the same device; we use them to build the noise schedule.\n",
        "\n",
        "    t_steps = (sigma_max ** (1 / rho) + step_indices / (num_steps - 1) * (sigma_min ** (1 / rho) - sigma_max ** (1 / rho))) ** rho\n",
        "    # Karras (EDM) sigma schedule: linearly interpolate between sigma_max^(1/ρ) and sigma_min^(1/ρ), then raise to ρ.\n",
        "    # Result: a monotone decreasing sequence from sigma_max to sigma_min, denser at small sigmas when ρ>1 (default: ρ=7).\n",
        "\n",
        "    t_steps = torch.cat([net.round_sigma(t_steps), torch.zeros_like(t_steps[:1])]) # t_N = 0\n",
        "    # Round each σ to the network’s supported grid (round_sigma) so preconditioning matches training.\n",
        "    # Append an extra 0 so the list length is num_steps+1 and the final step lands at zero noise.\n",
        "\n",
        "    # Main sampling loop.\n",
        "    x_next = latents.to(torch.float64) * t_steps[0]\n",
        "    # Initialize at the highest noise level: latents ~ N(0, I) -> x ~ N(0, sigma_max^2 I).\n",
        "\n",
        "    for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])): # 0, ..., N-1\n",
        "        x_cur = x_next\n",
        "\n",
        "        # Increase noise temporarily.\n",
        "        gamma = min(S_churn / num_steps, np.sqrt(2) - 1) if S_min <= t_cur <= S_max else 0\n",
        "        # Decide how much extra noise (“churn”) to add this step; capped by √2-1, active only on [S_min, S_max].\n",
        "\n",
        "        t_hat = net.round_sigma(t_cur + gamma * t_cur)\n",
        "        # Compute the “churned” sigma: t_hat = (1 + gamma) * t_cur, then round to the supported σ grid.\n",
        "\n",
        "        x_hat = x_cur + (t_hat ** 2 - t_cur ** 2).sqrt() * S_noise * randn_like(x_cur)\n",
        "        # Add just enough Gaussian noise so the total variance grows from t_cur^2 to t_hat^2 (scaled by S_noise).\n",
        "\n",
        "        # Euler step.\n",
        "        denoised = net(x_hat, t_hat, class_labels).to(torch.float64)\n",
        "        # In EDM, net(·, σ) returns an estimate of the clean image \\hat X_0 (pre/post-scaling internal).\n",
        "        # Use float64 for a bit more numerical stability.\n",
        "\n",
        "        # For the EDM probability-flow ODE, dx/dσ = (x - X0)/σ. Replacing X0 by denoised gives this slope.\n",
        "        d_cur = (x_hat - denoised) / t_hat\n",
        "\n",
        "        x_next = x_hat + (t_next - t_hat) * d_cur\n",
        "        # Explicit Euler update from σ = t_hat down to the next scheduled σ = t_next.\n",
        "\n",
        "        # Apply 2nd order correction.\n",
        "        if i < num_steps - 1:\n",
        "            denoised = net(x_next, t_next, class_labels).to(torch.float64)\n",
        "            d_prime = (x_next - denoised) / t_next\n",
        "            # Heun correction (prediction–correction): re-evaluate slope at the end of the interval.\n",
        "\n",
        "            x_next = x_hat + (t_next - t_hat) * (0.5 * d_cur + 0.5 * d_prime)\n",
        "            # Trapezoidal rule: average of start/end slopes times step size, applied from x_hat.\n",
        "\n",
        "    return x_next\n",
        "```\n",
        "\n",
        "The line\n",
        "```{Python}\n",
        "denoised = net(x_hat, t_hat, class_labels).to(torch.float64)\n",
        "```\n",
        "calls the denoiser ($D$) function which, under the hood, calls the $F$ network with the scaled input and scales the output, just like we have discussed a moment ago.\n",
        "\n",
        "In case you want to change the generator this is the function you want to work with.\n",
        "\n",
        "\n",
        "Now, let's check the number of function evaluations it makes:\n",
        "\n",
        "- for iterations 1..17 it calls the network in both Euler step and Heun correction\n",
        "- for the last iterations, it calls the network in the Euler step only.\n",
        "\n",
        "It makes 35 network calls and this is the value you will find throughout the Karras paper (eg. in Table 2).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VjH6Gj01_gHs"
      },
      "id": "VjH6Gj01_gHs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deterministic ODE Update in Euler Step\n",
        "\n",
        "In the deterministic ODE settging, i.e. there is no churn, `x_hat==x_cur` and the EDM makes the following update:\n",
        "```{Python}\n",
        "d_cur = (x_cur - denoised) / t_hat  \n",
        "x_next = x_cur + (t_next - t_hat) * d_cur\n",
        "```\n",
        "it can be further simplified as following:\n",
        "```{Python}\n",
        "x_next = x_cur + (t_next - t_hat) * (x_cur - denoised) / t_hat\n",
        "```\n",
        "\n",
        "$$\n",
        "X_{next} = X_{cur} + (\\sigma_{next} - \\sigma_{cur}) * \\frac{(X_{cur} - \\hat X_{cur})}{\\sigma_{cur}}\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "X_{next} = r X_{cur} + (1-r)\\hat X_{cur},\\quad\\quad\\text{with }\n",
        "r=\\frac{\\sigma_{next}}{\\sigma_{cur}}.\n",
        "$$\n",
        "\n",
        "Consider the standard discrete ancestral process with the denoiser parametrisation $\\hat X_{cur} = \\hat X_{cur}(X_{cur}) := D(X_{cur}, \\sigma_{cur})$:\n",
        "$$\n",
        "X_{next} = \\sigma_{next} \\left( \\sqrt{\\rho_{next}} - \\sqrt{\\rho_{cur}} \\sqrt{1 - \\eta_{next}^2} \\right) \\hat X_{cur} + \\frac{\\sigma_{next}}{\\sigma_{cur}} \\sqrt{1 - \\eta_{next}^2} X_{cur} + \\sigma_{next} \\eta_{next} \\tilde\\epsilon_{next},\n",
        "$$\n",
        "with (for all possible $t$ we consider):\n",
        "- $\\alpha_t$ the signal schedule,\n",
        "- $\\sigma_t$ the noise schedule,\n",
        "- $\\eta_t  \\geq 0$ the diffusion schedule,\n",
        "- SNR ratio $\\rho_t = \\frac{\\alpha_t^2}{\\sigma_t^2}$\n",
        "- $\\tilde \\epsilon_t \\sim N(0, I)$.\n",
        "\n",
        "It will be introduced in the next class formally and with more deatil, today take it for granted.\n",
        "\n",
        "If you set $\\alpha_{cur}=\\alpha_{next}=1$, $\\eta \\equiv 0$ (no diffusion, deterministic schedule), you will get exactly\n",
        "the EDM Euler update (**check it!**)\n",
        "\n",
        "$$\n",
        "X_{next} = r X_{cur} + (1-r)\\hat X_{cur},\\quad\\quad\\text{with }\n",
        "r=\\frac{\\sigma_{next}}{\\sigma_{cur}}.\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "x09FRhwMS8e-"
      },
      "id": "x09FRhwMS8e-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Executing Image Generation from the Container\n",
        "\n",
        "To calculate FID reliably you need to generate 50000 images.\n",
        "\n",
        "Execute the following SLURM script. Note there are **TWO** places where you specify the number of GPUs to be used:\n",
        "\n",
        "    {bash}\n",
        "    #!/bin/bash\n",
        "    #SBATCH --job-name=edm_generation\n",
        "    #SBATCH --nodes=1                     \n",
        "    #SBATCH --ntasks=1\n",
        "    #SBATCH --gres=gpu:1              <--------- HERE\n",
        "    #SBATCH --time=48:00:00\n",
        "    #SBATCH --account=g99-4302\n",
        "    #SBATCH --output=generate-%j.out\n",
        "\n",
        "    # Run generate.py inside the container using Apptainer and torchrun\n",
        "    apptainer exec --nv --bind \"$PWD\" ./edm_latest.sif \\\n",
        "      torchrun --standalone --nproc_per_node=1 \\   <------ AND HERE\n",
        "        generate.py \\\n",
        "        --outdir=out_50k \\\n",
        "        --seeds=0-49999 \\\n",
        "        --batch=128 \\\n",
        "        --steps=18 \\\n",
        "        --network=https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-cifar10-32x32-cond-vp.pkl\n",
        "\n",
        "Before running the final execution, it's a good idea to **calibrate the batch size** so that the **available GPU memory is fully utilized**, or at least used as efficiently as possible. In order to do that I would perform the short interactive run (beyond the scope of this class, read more on interactive running in [ICM help page on SLURM](https://kdm.icm.edu.pl/Tutorials/HPC-intro/slurm_intro/))\n",
        "\n",
        "    {bash}\n",
        "    srun -A g99-4302 -J edm_generation \\\n",
        "     -N 1 -n 1 --gres=gpu:1 \\\n",
        "     --time=48:00:00 \\\n",
        "     --output=generate-%j.out \\\n",
        "     --pty /bin/bash -l  # cluster: rysy\n",
        "\n",
        "    # after I get allocated to the particular computing node\n",
        "    apptainer exec --nv --bind \"$PWD\" ./edm_latest.sif \\\n",
        "      torchrun --standalone --nproc_per_node=1 \\\n",
        "        generate.py \\\n",
        "        --outdir=out_50k \\\n",
        "        --seeds=0-49999 \\\n",
        "        --batch=128 \\\n",
        "        --steps=18 \\\n",
        "        --network=https://nvlabs-fi-cdn.nvidia.com/edm/pretrained/edm-cifar10-32x32-cond-vp.pkl &\n",
        "      \n",
        "    # (note the asterisk & which makes the job go to the background,\n",
        "    #  so I can invoke nvidia-smi in the foreground)\n",
        "\n",
        "    nvidia-smi   # it let's me monitor the GPU load\n",
        "\n",
        "After the batch size has been established, you can put it into the SLURM script and execute the batch computation.\n",
        "\n",
        "The internet may be unavailable in some clusters from the computation nodes (not the case in ICM). In this case you could download the network and indicate the local storage with the `--network` argument (eg. `--network=downloaded_networks/edm-cifar10-32x32-cond-vp.pkl`)."
      ],
      "metadata": {
        "id": "NijtSY90W1HR"
      },
      "id": "NijtSY90W1HR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating FID\n",
        "\n",
        "The **Fréchet Inception Distance (FID)** is a widely used metric for evaluating the quality of images generated by generative models such as GANs and diffusion models.\n",
        "\n",
        "FID was introduced in the paper *GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium* by Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter, published in NeurIPS 2017 (~20k citations as of Oct 2025).\n",
        "\n",
        "\n",
        "Its main idea is to measure how close the **distribution of generated images** is to the **distribution of real images**, but since comparing high-dimensional image distributions directly is intractable, FID does this comparison in a **feature space** learned by a pretrained neural network — typically the Inception-v3 network.\n",
        "\n",
        "Specifically, both real and generated images are passed through the network, and the resulting feature vectors (usually from a late pooling layer) are modeled as **multivariate Gaussians** with estimated means and covariances $(\\mu_r, \\Sigma_r)$ and $(\\mu_g, \\Sigma_g)$. The FID is then computed as the **Fréchet distance** (or **2-Wasserstein distance**) between these two Gaussians:\n",
        "\n",
        "$$\n",
        "\\mathrm{FID} = \\|\\mu_r - \\mu_g\\|_2^2 + \\mathrm{Tr}\\!\\left(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\Sigma_g)^{1/2}\\right)\n",
        "$$\n",
        "\n",
        "Intuitively, it measures both the **difference in mean features** (capturing overall content) and **differences in covariance** (capturing diversity and structure).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SewvPycm_ljV"
      },
      "id": "SewvPycm_ljV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lower FID Is Better\n",
        "\n",
        "Lower FID values indicate that generated images are more similar to real ones, both in quality and variety.  \n"
      ],
      "metadata": {
        "id": "Pp0ZQUyVCt6w"
      },
      "id": "Pp0ZQUyVCt6w"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FID Calculation\n",
        "\n",
        "To calculate FID, execute the following SLURM script `fid.slurm`. The calculation is relatively fast, so I don't even use `torchrun` for parallelization of GPU computation (request one GPU only) nor did I calibrate the batch size (the default is 64, it seems)\n",
        "\n",
        "    {bash}\n",
        "    #!/bin/bash\n",
        "    #SBATCH --job-name=fid\n",
        "    #SBATCH --nodes=1                     \n",
        "    #SBATCH --ntasks=1\n",
        "    #SBATCH --gres=gpu:1              \n",
        "    #SBATCH --time=48:00:00\n",
        "    #SBATCH --account=g99-4302\n",
        "    #SBATCH --output=fid-%j.out\n",
        "\n",
        "    # Run fid.py inside the container using Apptainer\n",
        "    apptainer exec --nv --bind \"$PWD\" ./edm_latest.sif \\\n",
        "        python fid.py calc --images=out_50k \\\n",
        "            --ref=https://nvlabs-fi-cdn.nvidia.com/edm/fid-refs/cifar10-32x32.npz\n",
        "\n",
        "`https://nvlabs-fi-cdn.nvidia.com/edm/fid-refs/cifar10-32x32.npz` contains precomputed Inception-V3 scores (feature statistics) for the original CIFAR-10 dataset.\n",
        "\n",
        "The internet may be unavailable in some clusters during computations (not the case in ICM). Then you can download the statistics and download the Inception-V3 network itself. For the statistics, you can indicate the local storage with the `--ref` argument (eg. `--ref=fid/cifar10-32x32.npz`, however for the Inception-V3 network you will need to make a chirurgic change in the code in [this line](https://github.com/NVlabs/edm/blob/main/fid.py#L34).\n",
        "\n",
        "    {bash}\n",
        "    szymon@rysy ~/edm $ sbatch fid.slurm\n",
        "    Submitted batch job 135531\n",
        "    szymon@rysy ~/edm $ squeue\n",
        "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
        "            127248       gpu setup_en   ljanis PD       0:00      1 (AssocGrpCPUMinutesLimit)\n",
        "            135531       gpu      fid   szymon  R       0:02      1 rysy-n3\n",
        "            134837       gpu      OLA tomaszsu  R 13-07:04:02      1 rysy-n6\n",
        "            135529       gpu sys/dash    mdzik  R      58:05      1 rysy-n3\n",
        "            135515        ve     bash   herman  R 1-06:31:21      1 pbaran\n",
        "\n",
        "    # After 1:30 minutes\n",
        "\n",
        "    szymon@rysy ~/edm $ more fid-135531.out\n",
        "    INFO:    underlay of /etc/localtime required more than 50 (94) bind mounts\n",
        "    INFO:    underlay of /usr/bin/nvidia-smi required more than 50 (471) bind mounts\n",
        "    13:4: not a valid test operator: (\n",
        "    13:4: not a valid test operator: 530.30.02\n",
        "    Loading dataset reference statistics from \"https://nvlabs-fi-cdn.nvidia.com/edm/fid-refs/cifar10-32x32.npz\"...\n",
        "    Loading Inception-v3 model...\n",
        "    Loading images from \"out_50k\"...\n",
        "    /opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Plea\n",
        "    se be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
        "      warnings.warn(_create_warning_msg(\n",
        "    Calculating statistics for 50000 images...\n",
        "     5%|▍         | 39/782 [00:09<00:58, 12.75batch/s]\n",
        "    10%|█         | 79/782 [00:12<01:10,  9.95batch/s]\n",
        "    15%|█▍        | 117/782 [00:15<00:51, 12.88batch/s]\n",
        "    20%|█▉        | 155/782 [00:18<00:48, 13.01batch/s]\n",
        "    25%|██▍       | 193/782 [00:21<00:45, 12.89batch/s]\n",
        "    29%|██▉       | 229/782 [00:24<00:42, 12.64batch/s]\n",
        "    34%|███▍      | 265/782 [00:27<00:40, 12.61batch/s]\n",
        "    38%|███▊      | 301/782 [00:30<00:37, 12.88batch/s]\n",
        "    43%|████▎     | 335/782 [00:32<00:34, 12.93batch/s]\n",
        "    47%|████▋     | 369/782 [00:35<00:31, 12.92batch/s]\n",
        "    52%|█████▏    | 403/782 [00:38<00:29, 12.99batch/s]\n",
        "    56%|█████▌    | 437/782 [00:40<00:26, 13.14batch/s]\n",
        "    60%|██████    | 471/782 [00:43<00:24, 12.95batch/s]\n",
        "    65%|██████▍   | 503/782 [00:45<00:21, 13.03batch/s]\n",
        "    69%|██████▊   | 535/782 [00:48<00:19, 12.88batch/s]\n",
        "    73%|███████▎  | 567/782 [00:50<00:16, 12.95batch/s]\n",
        "    77%|███████▋  | 599/782 [00:53<00:14, 12.94batch/s]\n",
        "    81%|████████  | 631/782 [00:55<00:11, 12.94batch/s]\n",
        "    85%|████████▍ | 661/782 [00:58<00:09, 12.94batch/s]\n",
        "    89%|████████▊ | 691/782 [01:00<00:07, 12.93batch/s]\n",
        "    92%|█████████▏| 723/782 [01:02<00:04, 12.83batch/s]\n",
        "    96%|█████████▌| 751/782 [01:05<00:02, 12.88batch/s]\n",
        "    100%|█████████▉| 781/782 [01:07<00:00, 13.35batch/s]\n",
        "    ▒▒████| 782/782 [01:12<00:00, 10.73batch/s]\n",
        "    Calculating FID...\n",
        "    1.96999\n",
        "\n"
      ],
      "metadata": {
        "id": "m8-pgW48dI64"
      },
      "id": "m8-pgW48dI64"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}