{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SzymonNowakowski/Machine-Learning-2025/blob/master/Lab15-EDM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 15 - EDM, Working in ICM\n",
        "\n",
        "### Author: Szymon Nowakowski"
      ],
      "metadata": {
        "id": "xl_-W_aXqjJ2"
      },
      "id": "xl_-W_aXqjJ2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Today I will talk about using methods you can find in the following paper:\n",
        "\n",
        "Karras, T., Aittala, M., Aila, T., & Laine, S. (2022). Elucidating the Design Space of Diffusion-Based Generative Models. In Proceedings of the Neural Information Processing Systems (NeurIPS) Conference.\n",
        "https://arxiv.org/abs/2206.00364\n",
        "\n",
        "### EDM Codebase\n",
        "\n",
        "https://github.com/NVlabs/edm\n",
        "\n",
        "Although 3 years old, the codebase and the precomputed networks it provides are still considered the state-of-the-art for generating images."
      ],
      "metadata": {
        "id": "q90PnkdyThfK"
      },
      "id": "q90PnkdyThfK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Presentation Plan\n",
        "\n",
        "## Cluster Computation (ICM Example)\n",
        "\n",
        "## Working with Containers\n",
        "\n",
        "## Best (Programming) Practices\n",
        "\n",
        "## EDM\n",
        "\n",
        "### Training\n",
        "\n",
        "### Generating Images\n",
        "\n",
        "### Calculating FID"
      ],
      "metadata": {
        "id": "Ceu8ykzgYSu4"
      },
      "id": "Ceu8ykzgYSu4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cluster Computation (ICM Example)"
      ],
      "metadata": {
        "id": "UZa709sKjXCr"
      },
      "id": "UZa709sKjXCr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## SLURM\n",
        "\n",
        "In ICM they use ***SLURM***. SLURM stands for **Simple Linux Utility for Resource Management** — it's a workload manager widely used on **HPC (High-Performance Computing)** clusters to schedule and manage jobs.\n",
        "\n",
        "It handles job submission, queuing, scheduling, monitoring, and resource allocation.\n",
        "\n",
        "In SLURM each command starts with a letter `s`:\n",
        "\n",
        "    {bash}\n",
        "    # Submit a job\n",
        "    sbatch myjob.sh\n",
        "\n",
        "    # Check job queue\n",
        "    squeue -u $USER\n",
        "\n",
        "    # Check all jobs\n",
        "    squeue\n",
        "\n",
        "    # Cancel a job\n",
        "    scancel 12345\n",
        "\n",
        "\n",
        "I will show you an example of a SLURM batch job file  (`myjob.sh` in examples above) in a moment.\n",
        "\n",
        "The great resource is the [ICM help page on SLURM](https://kdm.icm.edu.pl/Tutorials/HPC-intro/slurm_intro/).\n",
        "\n",
        "The available resouces i.e. configurations, GPUs, available memory can be checked at the [ICM help page on computing resources](https://kdm.icm.edu.pl/Zasoby/komputery_w_icm.pl/).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UWP80vZyaNMa"
      },
      "id": "UWP80vZyaNMa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSF\n",
        "\n",
        "\n",
        "There exist other computing HPC environments, the most popular alternative to SLURM being ***LSF*** (**Load Sharing Facility**), developed originally by Platform Computing (now delivered by IBM).\n",
        "\n",
        "In LSF each command starts with a letter `b`:\n",
        "\n",
        "    {bash}\n",
        "    # Submit a job\n",
        "    bsub < myjob.lsf\n",
        "\n",
        "    # Check your jobs\n",
        "    bjobs\n",
        "\n",
        "    # Check all jobs\n",
        "    bjobs -u all\n",
        "\n",
        "    # Show information about available queues\n",
        "    bqueues\n",
        "\n",
        "    # Kill a job\n",
        "    bkill 12345\n"
      ],
      "metadata": {
        "id": "UXTc8oHrjT6n"
      },
      "id": "UXTc8oHrjT6n"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with Containers\n",
        "\n",
        "In many HPC environments (including ICM), it is hard to install your own dependencies. While it is possible to install additional Python packages, it is sometimes hard or impossible to install additionall binaries.\n",
        "\n",
        "In another cluster I work with in WIM, there is no Internet availability at all, so one cannot even install additional packages.\n",
        "\n",
        "**The way out is using the containers.** But there is another obstacle: you cannot run Docker directly for security reasons in a cluster.  \n",
        "\n",
        "Instead, you typically **build docker containers locally** (on your workstation or laptop) and then **transfer them** to the cluster, where they can be rebuild docker containers to *Apptainer* (formerly Singularity).\n",
        "\n",
        "Below are the steps to create, save, and rebuild a Docker container image."
      ],
      "metadata": {
        "id": "VbufEDhgozJY"
      },
      "id": "VbufEDhgozJY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## `Dockerfile`\n",
        "\n",
        "The `Dockerfile` describes your environment — e.g., which base image to use, which Python packages to install, etc.\n",
        "\n",
        "    {bash}\n",
        "    # Example Dockerfile\n",
        "    FROM pytorch/pytorch:2.3.0-cuda12.1-cudnn8-runtime\n",
        "\n",
        "    # Set working directory\n",
        "    WORKDIR /workspace\n",
        "\n",
        "    # Copy your project\n",
        "    COPY . /workspace\n",
        "\n",
        "    # Install extra dependencies\n",
        "    RUN pip install -r requirements.txt\n",
        "\n",
        "    # Default command\n",
        "    CMD [\"python\", \"train.py\"]\n",
        "\n"
      ],
      "metadata": {
        "id": "RIDQyXrFo1ZR"
      },
      "id": "RIDQyXrFo1ZR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Docker Image\n",
        "\n",
        "    {bash}\n",
        "    docker build -t myproject:latest .\n",
        "\n",
        "This creates a local image named `myproject:latest`. Note the `.` (dot) indicating the `Dockerfile` file in the current directory.\n",
        "\n",
        "Now, when you run\n",
        "\n",
        "    {bash}\n",
        "    docker run myproject\n",
        "what happens is that the `python train.py` gets executed within the container.\n",
        "\n",
        "There can only be one `CMD` instruction in a `Dockerfile` (if there are multiple, only the last one is used).\n",
        "\n",
        "Often, `CMD` is used to launch the main script or application of the container. This command can be overwritten and you can run some other code (you can even pass arguments) by executing\n",
        "\n",
        "    {bash}\n",
        "    docker run myproject python some_other_code.py --epochs 10 --lr 1e-3"
      ],
      "metadata": {
        "id": "NTvBbZrlo5pg"
      },
      "id": "NTvBbZrlo5pg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the Image to a `.tar` File\n",
        "\n",
        "    {bash}\n",
        "    docker save myproject:latest -o myproject_latest.tar\n",
        "    ls -lh myproject_latest.tar\n",
        "\n",
        "\n",
        "What happens is that the file `myproject_latest.tar` gets written to the current directory."
      ],
      "metadata": {
        "id": "IorczX41pPdi"
      },
      "id": "IorczX41pPdi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Copying the container image to ICM\n",
        "\n",
        "Use `scp` (secure copy) to transfer the image file to your home directory at ICM.  \n",
        "Replace `username` with your ICM login name.\n",
        "\n",
        "    {bash}\n",
        "    scp myproject_latest.tar username@hpc.icm.edu.pl:/lu/tetyda/home/username/\n",
        "\n",
        "`/lu/tetyda/home/username/` is a global path of your rysy home directory visible from `hpc.icm.edu.pl` server.\n"
      ],
      "metadata": {
        "id": "caBfxZDVljIm"
      },
      "id": "caBfxZDVljIm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting the Image to Apptainer Format on ICM\n",
        "\n",
        "Here is an example of the SLURM configuration file that will rebuild the docker container `myproject_latest.tar` into apptainer's `myproject_latest.sif`:\n",
        "\n",
        "    {bash}\n",
        "    # after `ssh`-ing to RYSY\n",
        "    more rebuild_container.slurm\n",
        "\n",
        "    #!/bin/bash\n",
        "    #SBATCH --job-name=docker2apptainer\n",
        "    #SBATCH --nodes=1\n",
        "    #SBATCH --ntasks=1\n",
        "    #SBATCH --gres=gpu:1\n",
        "    #SBATCH --time=48:00:00\n",
        "    #SBATCH --account=g99-4302\n",
        "    #SBATCH --output=slurm-%j.out\n",
        "\n",
        "    export APPTAINER_TMPDIR=/home/$USER/tmp\n",
        "\n",
        "    apptainer build ./myproject_latest.sif docker-archive:///home/szymon/edm/myproject_latest.tar\n",
        "\n",
        "Now we shall go through it line by line.\n",
        "\n",
        "- `export APPTAINER_TMPDIR=/home/$USER/tmp` - this lines ensures that there is enough space on the output for temporary fies. Obviously, you need to create the `~/tmp` directory first.\n",
        "- Note the triple `///` - it costed me a few days to figure it out. These days, ChatGPT can supply such hints instantly.\n",
        "\n",
        "Now one needs to execute on RYSY\n",
        "\n",
        "    {bash}\n",
        "    sbatch rebuild_container.slurm\n",
        "    squeue\n",
        "\n",
        "to see something like\n",
        "\n",
        "    {bash}\n",
        "    JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
        "    127248       gpu setup_en   ljanis PD       0:00      1 (AssocGrpCPUMinutesLimit)\n",
        "    134837       gpu      OLA tomaszsu  R 12-08:46:28     1 rysy-n6\n",
        "    134838       gpu docker2a   szymon  R       0:18      1 rysy-n1\n",
        "    135515        ve     bash   herman  R    8:13:47      1 pbaran\n",
        "\n",
        "And after a few minutes or hours, depending on the docker file size, one gets `myproject_latest.sif` file written to the current directory. **This is the apptainer container file**.\n",
        "\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "mxvrzmorqU1L"
      },
      "id": "mxvrzmorqU1L"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDM Example\n",
        "\n",
        "The original EDM `Dockerfile` from `https://github.com/NVlabs/edm/blob/main/Dockerfile` is the following\n",
        "\n",
        "    {bash}\n",
        "    # Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
        "    #\n",
        "    # This work is licensed under a Creative Commons\n",
        "    # Attribution-NonCommercial-ShareAlike 4.0 International License.\n",
        "    # You should have received a copy of the license along with this\n",
        "    # work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n",
        "\n",
        "    FROM nvcr.io/nvidia/pytorch:22.10-py3\n",
        "\n",
        "    ENV PYTHONDONTWRITEBYTECODE 1\n",
        "    ENV PYTHONUNBUFFERED 1\n",
        "\n",
        "    RUN pip install imageio imageio-ffmpeg==0.4.4 pyspng==0.1.0\n",
        "\n",
        "    WORKDIR /workspace\n",
        "\n",
        "    RUN (printf '#!/bin/bash\\nexec \\\"$@\\\"\\n' >> /entry.sh) && chmod a+x /entry.sh\n",
        "    ENTRYPOINT [\"/entry.sh\"]\n",
        "\n",
        "The original container file gave me errors (I was not able to execute the code from within it,  no doubt due to broken dependencies along the way - the code is 3 years old), and I had no access to the original NVidia image which would have obviously worked fine. I needed to update `pillow` package in my fork **`https://github.com/SzymonNowakowski/edm/blob/main/Dockerfile`:**\n",
        "\n",
        "    {bash}\n",
        "    # Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
        "    #\n",
        "    # This work is licensed under a Creative Commons\n",
        "    # Attribution-NonCommercial-ShareAlike 4.0 International License.\n",
        "    # You should have received a copy of the license along with this\n",
        "    # work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n",
        "\n",
        "    FROM nvcr.io/nvidia/pytorch:22.10-py3\n",
        "\n",
        "    ENV PYTHONDONTWRITEBYTECODE 1\n",
        "    ENV PYTHONUNBUFFERED 1\n",
        "\n",
        "    RUN pip install imageio imageio-ffmpeg==0.4.4 pyspng==0.1.0\n",
        "    RUN pip install --upgrade pillow\n",
        "\n",
        "    WORKDIR /workspace\n",
        "\n",
        "    RUN (printf '#!/bin/bash\\nexec \\\"$@\\\"\\n' >> /entry.sh) && chmod a+x /entry.sh\n",
        "    ENTRYPOINT [\"/entry.sh\"]\n",
        "\n",
        "From there the standard sequence would let me build the docker container and ship it to ICM:\n",
        "\n",
        "    {bash}\n",
        "    docker build -t edm:latest .\n",
        "    docker save edm:latest -o edm_latest.tar\n",
        "    scp edm_latest.tar username@hpc.icm.edu.pl:/lu/tetyda/home/username/\n",
        "\n",
        "Now we need to rewrite the SLURM script to include different filenames (it assumes the `~/tmp` directory is available - if not - create it!) so it looks like this:\n",
        "\n",
        "    {bash}\n",
        "    # while on your terminal\n",
        "    ssh username@hpc.icm.edu.pl\n",
        "\n",
        "    # while on HPC computer\n",
        "    ssh rysy\n",
        "\n",
        "    # while on RYSY computer\n",
        "    more rebuild_container.slurm\n",
        "    #!/bin/bash\n",
        "    #SBATCH --job-name=docker2apptainer\n",
        "    #SBATCH --nodes=1\n",
        "    #SBATCH --ntasks=1\n",
        "    #SBATCH --gres=gpu:1\n",
        "    #SBATCH --time=48:00:00\n",
        "    #SBATCH --account=g99-4302\n",
        "    #SBATCH --output=slurm-%j.out\n",
        "\n",
        "    export APPTAINER_TMPDIR=/home/$USER/tmp\n",
        "\n",
        "    apptainer build ./edm_latest.sif docker-archive:///home/szymon/edm/edm_latest.tar\n",
        "\n",
        "Now one needs to execute\n",
        "\n",
        "    {bash}\n",
        "    sbatch rebuild_container.slurm\n",
        "    squeue\n",
        "\n",
        "to see something like\n",
        "\n",
        "    {bash}\n",
        "    JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
        "    127248       gpu setup_en   ljanis PD       0:00      1 (AssocGrpCPUMinutesLimit)\n",
        "    134837       gpu      OLA tomaszsu  R 12-08:46:28     1 rysy-n6\n",
        "    134838       gpu docker2a   szymon  R       0:18      1 rysy-n1\n",
        "    135515        ve     bash   herman  R    8:13:47      1 pbaran\n",
        "\n",
        "And after a few hours the file `edm_latest.sif` gets written to the current directory.\n",
        "\n",
        "**This is the apptainer container file we shall need**."
      ],
      "metadata": {
        "id": "zUs4zB_uMSGi"
      },
      "id": "zUs4zB_uMSGi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best (Programming) Practices"
      ],
      "metadata": {
        "id": "D8Z0owLY-2o7"
      },
      "id": "D8Z0owLY-2o7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Configure Your `ssh` and `scp` Connection\n",
        "\n",
        "As you need a stable `ssh` and `scp` connection with the cluster, you will need to configure it with a file `~/.ssh/config`. An example of such a config file I use:\n",
        "\n",
        "    {bash}\n",
        "    cat ~/.ssh/config\n",
        "\n",
        "    Host icm\n",
        "        ForwardX11 yes\n",
        "        ForwardAgent yes\n",
        "        UserKnownHostsFile ~/.ssh/known_hosts\n",
        "        Hostname hpc.icm.edu.pl\n",
        "        LocalForward 8022 rysy:22\n",
        "        ServerAliveInterval 120\n",
        "        ServerAliveCountMax 2\n",
        "        User szymon\n",
        "\n",
        "    Host fizyk1.fuw.edu.pl\n",
        "        Hostname fizyk1.fuw.edu.pl\n",
        "        ServerAliveInterval 120\n",
        "        ServerAliveCountMax 2\n",
        "        User snowakowski\n",
        "\n",
        "    Host dg1\n",
        "        ForwardX11 yes\n",
        "        ForwardAgent yes\n",
        "        UserKnownHostsFile ~/.ssh/known_hosts\n",
        "        Hostname 10.21.2.118\n",
        "        LocalForward 5900 localhost:5900\n",
        "        LocalForward 8000 localhost:8000\n",
        "        LocalForward 8001 localhost:8001\n",
        "        LocalForward 8008 localhost:8008\n",
        "        LocalForward 8888 localhost:8888\n",
        "        LocalForward 6006 localhost:6006\n",
        "        LocalForward 1433 localhost:1433\n",
        "        RequestTTY yes\n",
        "        ServerAliveInterval 120\n",
        "        ServerAliveCountMax 2\n",
        "        ProxyJump  fizyk1.fuw.edu.pl:22\n",
        "        User szym\n",
        "\n",
        "- `ServerAliveInterval 120` - instructs your SSH client to send a small, encrypted keep-alive message to the server every 120 seconds (2 minutes).\n",
        "So every 2 minutes, your client pings the server silently to say “I'm still here.”\n",
        "\n",
        "- `ServerAliveCountMax 2` - defines how many unanswered keep-alive messages the client will tolerate before disconnecting.\n",
        "With `ServerAliveCountMax 2`, if the server fails to respond to two consecutive keep-alives, the client assumes the connection is dead and terminates it.\n",
        "\n",
        "The `dg1` via `fizyk1` connection is to show you how to configure the proxy jump.\n",
        "\n",
        "Now, instead of\n",
        "\n",
        "    {bash}\n",
        "    # while on your terminal\n",
        "    ssh -l snowakowski fizyk1.fuw.edu.pl\n",
        "\n",
        "    # while on fizyk1\n",
        "    ssh -l szym 10.21.2.118\n",
        "\n",
        "You can invoke simply one-step connection and with a server name (instead of the IP) and without a username!\n",
        "\n",
        "    {bash}\n",
        "    ssh dg1\n",
        "\n",
        "The same applies to `scp`.\n",
        "\n",
        "**Next I will show you how to enable the tab-completion of remote paths in `scp`**.\n"
      ],
      "metadata": {
        "id": "8l-DphqarxQp"
      },
      "id": "8l-DphqarxQp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Configure Your `ssh` and `scp` Certificate\n",
        "\n",
        "As you will frequently need to `ssh` and `scp` to and from the cluster, the second thing to consider is to set up a password-less connection. You not only skip typing your password every time, but you also unlock extra conveniences such as **tab-completion of remote paths** in `scp`.\n",
        "\n",
        "To generate a public/private key pair execute:\n",
        "\n",
        "    {bash}\n",
        "    ssh-keygen\n",
        "\n",
        "It generates:\n",
        "- your private key: `~/.ssh/id_rsa`\n",
        "- your public key: `~/.ssh/id_rsa.pub`\n",
        "\n",
        "To copy the public key to the server (for instance to ICM) do it with\n",
        "\n",
        "    {bash}\n",
        "    ssh-copy-id icm\n",
        "\n",
        "(assuming you have your `~/.ssh/config` file already created).\n",
        "\n",
        "After that\n",
        "\n",
        "    {bash}\n",
        "    ssh icm\n",
        "\n",
        "gets you to the `hpc.icm.edu.pl` server with only the OTP (one time password). That you cannot avoid, or at least I don't know how. In `hpc`, you can set up a direct certificate-based (pasword-less) connection to `rysy`, to be able to type only (without the password).\n",
        "\n",
        "    {bash}\n",
        "    ssh rysy"
      ],
      "metadata": {
        "id": "GvwDdgXq-5sm"
      },
      "id": "GvwDdgXq-5sm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Smart Use of GitHub\n",
        "\n",
        "Using GitHub is always a good idea as it provides independent backup and code versioning without much added effort. Showing how to use `git` is much beyond the scope of this class, but I want to make some points along the way.\n",
        "\n",
        "- As we want to extend EDM"
      ],
      "metadata": {
        "id": "i6lPkN5rR98u"
      },
      "id": "i6lPkN5rR98u"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SMpFhnM8SHSp"
      },
      "id": "SMpFhnM8SHSp"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}